## DS-SUMMER-LLM
# finetuned BLOOM to produce [BLOOMZ] & our guide is here[https://github.com/bigscience-workshop/xmtf#bloomz].
# [https://huggingface.co/bigscience/bloomz]
# paper "BLOOM: A 176B-Parameter Open-Access Multilingual Language Model" [https://arxiv.org/abs/2211.05100]
# The architecture of BLOOM is essentially similar to GPT3 (auto-regressive model for next token prediction), but has been trained on 46 different languages and 13 programming languages. 
# Several smaller versions of the models have been trained on the same dataset. 
BLOOM’sdevelopmentwascoordinatedbyBigScience, an openresearch collaboration whose goal was thepublic release of an LLM.
The project started after being awarded by GENCI a compute grant on its JeanZay supercomputer at IDRIS/CNRS.
It was initially built around a concerted effort from HuggingFace and the French NLP community (the“foundingmembers”),and quickly opened up to growintoabroaderinternational collaborationtosupportitsaimsoflinguistic,geographical,andscientificdiversity.In theend,over1200peopleregisteredasparticipantsinBigScienceandweregivenaccess toitscommunicationchannels.Theyhadbackgroundnotonlyinmachinelearningand computerscience,butalsolinguistics,statistics,socio-culturalanthropology,philosophy, law,andotherfields.Ofthose,hundredsofindividualshavedirectlycontributedtoone oftheproject’sreleasedartifacts.Whilethelargestnumberofparticipantsultimately originatedfromtheUS,38countrieswererepresented.\

* petals: decentralized BLOOM-176B inference and fine-tuning in Google Colab[https://colab.research.google.com/drive/1Ervk6HPNS6AYVr3xVdQnY5a-TjjmLCdQ?usp=sharing]]
* chatbot link: [http://chat.petals.ml/]
* reference:[https://huggingface.co/docs/transformers/model_doc/bloom]