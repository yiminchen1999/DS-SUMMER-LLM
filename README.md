## DS-SUMMER-LLM
* finetuned BLOOM to produce [BLOOMZ] & our guide is here[https://github.com/bigscience-workshop/xmtf#bloomz].
* [https://huggingface.co/bigscience/bloomz]
* paper "BLOOM: A 176B-Parameter Open-Access Multilingual Language Model" [https://arxiv.org/abs/2211.05100]
* The architecture of BLOOM is essentially similar to GPT3 (auto-regressive model for next token prediction), but has been trained on 46 different languages and 13 programming languages. 
* Several smaller versions of the models have been trained on the same dataset. 
 
* petals: decentralized BLOOM-176B inference and fine-tuning in Google Colab[https://colab.research.google.com/drive/1Ervk6HPNS6AYVr3xVdQnY5a-TjjmLCdQ?usp=sharing]]
* chatbot link: [http://chat.petals.ml/]
* reference:[https://huggingface.co/docs/transformers/model_doc/bloom]